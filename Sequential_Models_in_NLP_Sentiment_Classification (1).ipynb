{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaFSkUu0fzm"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OudB5by50jlI"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "### Dataset\n",
        "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
        "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
        "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "Command to import data\n",
        "- `from tensorflow.keras.datasets import imdb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34-Y3nRKXdO"
      },
      "source": [
        "### Import the data (4 Marks)\n",
        "- Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxfwbrbuKbk2",
        "outputId": "da7bca3f-83f3-4efd-9ff0-5918e3d5a9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldivBO4LTbP"
      },
      "source": [
        "### Pad each sentence to be of same length (4 Marks)\n",
        "- Take maximum sequence length as 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E808XB4tLtic"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "X_train = sequence.pad_sequences(X_train_orig, maxlen=300)\n",
        "X_test = sequence.pad_sequences(X_test_orig, maxlen=300)\n",
        "y_train=y_train_orig\n",
        "y_test=y_test_orig"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ma88zGCBQ8R"
      },
      "source": [
        "Since the split of train and test is by default 50:50. Merging the data to split it into 75:25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBApgl77BOnk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X=np.concatenate((X_train,X_test),axis=0)\n",
        "Y=np.concatenate((y_train,y_test),axis=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFFCrybMSXz"
      },
      "source": [
        "### Print shape of features & labels (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOcyRtZfMYZd"
      },
      "source": [
        "Number of review, number of words in each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdMCUPr7RaCm",
        "outputId": "00076577-a420-4677-f789-1c89cda6ed01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Total number of reviews in Train Data is :',len(X_train))\n",
        "length = [len(i) for i in X_train]\n",
        "print('Average words in each review in Train Data is :',np.mean(length))\n",
        "print('Shape of Train Data:',X_train.shape)\n",
        "print(\"Total Unique Words in Train Data:\", len(np.unique(np.hstack(X_train))))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of reviews in Train Data is : 37500\n",
            "Average words in each review in Train Data is : 300.0\n",
            "Shape of Train Data: (37500, 300)\n",
            "Total Unique Words in Train Data: 9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVHeKOWyJiG",
        "outputId": "87e1eeeb-c59d-4d79-83bd-03e6dd427bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Total number of reviews in Test Data is :',len(X_test))\n",
        "length = [len(i) for i in X_test]\n",
        "print('Average words in each review in Test Data is :',np.mean(length))\n",
        "print('Shape of Test Data:',X_test.shape)\n",
        "print(\"Total Unique Words in Test Data:\", len(np.unique(np.hstack(X_test))))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of reviews in Test Data is : 12500\n",
            "Average words in each review in Test Data is : 300.0\n",
            "Shape of Test Data: (12500, 300)\n",
            "Total Unique Words in Test Data: 9996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNk5sDvMr3j"
      },
      "source": [
        "Number of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z00-mYgMoKv",
        "outputId": "e02b7263-143c-4606-8734-6c3739c3f98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "print('Total number of Lables in Train Data is :',len(y_train))\n",
        "print('Unique Lables in Train Data are :',np.unique(y_train))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Lables in Train Data is : 37500\n",
            "Unique Lables in Train Data are : [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7f5tPeaMxti",
        "outputId": "219cd8fa-f242-4c97-a654-23ac900c4e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Total number of Lables in Test Data is :',len(y_test))\n",
        "print('Unique Lables in Test Data are :',np.unique(y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Lables in Test Data is : 12500\n",
            "Unique Lables in Test Data are : [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdXPWuOmNEbh"
      },
      "source": [
        "### Print value of any one feature and it's label (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLEdeFmNZfR"
      },
      "source": [
        "Train Feature value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf1Ezw25MhE4",
        "outputId": "d272550f-f91a-4aca-e87f-b8b6d060dcd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Sample Train Feature Value: ',X_train[5])\n",
        "if y_train[5]:\n",
        "  review='Positive'\n",
        "else:\n",
        "  review='Negative'\n",
        "print('Label for the sample train data is: ',review)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Train Feature Value:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   13   64  219    2  280  220 1088  153  596   21   12\n",
            "   47 2368   72  237   10   10   31  720  572 2677   11  330    5  100\n",
            "   64   28   77 3357   34    6   87  167   17  488    2 4064    9   10\n",
            "   10    4 2008    4 3324    7  479    5    6 4887 1272    5 7790   32\n",
            " 2866   23  711    2 5202   35  511    7    2   47    2    4    2    2\n",
            "   27   87 5144    8    2   29    9  579   29  215 3552   27  577    2\n",
            "    8 8393   11  661    8 3235    4 4239   18    4    2    7    4 3207\n",
            " 5159   29 2721    2   21  266  187    5 3351   27  322    2    8  721\n",
            "   68  577    8    4 3888 1250   11  661    8 2275    4  833    7   32\n",
            " 5197    2   10   10    2    5    2 3854  169   46   44    4 3552    5\n",
            " 3980    8    4 6183   18 7525    5 4546 2086    4 2606    2 2147   15\n",
            "   27  403   47   77  343   11   14    2    2   96   29 9099    6 2190\n",
            "    8  376    2   15   29   80 2910   41   10   10    4  324    4  370\n",
            " 6924    4  107  185   84  209  267 2555   33  257   85   36    2   68\n",
            " 1949    5    4 5340    7  349   15    2   68  447  663    5 4021   18\n",
            "  325 1087   36    2   33  257   85    5   18   31  561   75  235  199\n",
            "   68  671    5  936    5    4    2  549   34    4  370 3159   15   11\n",
            "  160   58  160  273   36  382  100  119  257   85    5   30 1021   12\n",
            "    9    6 2455    5  619    2   15 6212   64   18   35 4181   10   10\n",
            "   51  458   51  370   51 2804]\n",
            "Label for the sample train data is:  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQh0OeHMQ-Zo"
      },
      "source": [
        "Test Data Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94DqljsxQ9PG",
        "outputId": "58a21b22-583e-4a12-ed63-a252c7655a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Sample Test Feature Value: ',X_test[5])\n",
        "if y_test[5]:\n",
        "  review='Positive'\n",
        "else:\n",
        "  review='Negative'\n",
        "print('Label for the sample test data is: ',review)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Test Feature Value:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    1   92 1632    8    4 5328  425 1985\n",
            "  255    4 6645   26   73  573   18 1489   35 4268   23  383    5 1006\n",
            "  120  779 2629   11   68  189  108   21   14   31  133    9   43    6\n",
            "  227   99   76 1985  255  186    8   28    6 2928  383  136    2  125\n",
            "   19    4  425  109  170  932    5 4653  880   41 1943  253    4   86\n",
            "  171  211   21    6 3139  234    7   14  461   55 7654  946    2   24\n",
            "   60    6 1985   21    6    2 2284  625    2   16    2   18    2    4\n",
            "   22  191   60 1197   94 1163   19    4   86  747  234    6 2763  112\n",
            " 7063 2467  189   13  197   13   16   11   18    6 1157  356  103  134\n",
            "    8    6  247  338  109 2078    7    4  668  112 3727 5233    5 3810\n",
            "    8    6 1060  708   33    4  130    4  167 5692   14    9  448   23\n",
            "    6  283   65 1243   32  208   10   10    8   30 1257   50   26    6\n",
            "  171  441 1409    7    4  365    4  425 1985  255    9    6    2    5\n",
            "   50   26    6  171  327  599  302    5 2516  139  190  134  413  247\n",
            " 7654  946   82    4 2278  807    9  184  642   11    6 1250 1597  262\n",
            "    4 3257   91 7030 1042   29 6539   23    4  292  472    4   22  461\n",
            "  184 2227  702  225   57  779  114   42  233  334   66   94    6  902\n",
            "   88  886    6 2786   13  421    4   22   69   49  147  986 1985  255\n",
            "    9  230 5585  550  728    5    6  227   99 1149   60   34    4 1553\n",
            "    7    4 2228  512  470  158]\n",
            "Label for the sample test data is:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cof4LSxNxuv"
      },
      "source": [
        "### Decode the feature value to get original sentence (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oiAyPZOkJD"
      },
      "source": [
        "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcpDfMKRdEN"
      },
      "source": [
        "Sample Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clsk-yK8OtzD"
      },
      "source": [
        "import re\n",
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRgOD5S2Uuvd"
      },
      "source": [
        "Now use the dictionary to get the original words from the encodings, for a particular sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ504QDORwxj",
        "outputId": "d915d0bc-5dbd-4a19-d013-d8f3755a30d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoded = \" \".join( [reverse_index.get(i - 3, \"\") for i in X_train[6]] )\n",
        "print('Sample Feature value of Training Data:') \n",
        "print(decoded.lstrip())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Feature value of Training Data:\n",
            "only a handful of the segments are engaging here a segment with a garage attendant from  is heartbreaking one with   bob  makes its point twist by twist until the final shot  things br br the problem with this movie is that only a few of the clips  paris the others are so  shot in theme tone  production that you may as well be watching the years best commercials 2006 it's really all over the place it doesn't develop over it's running time and nothing  the directors in no  successfully joins the pieces tedium sets in i'm at the one hour twenty minute point and  wood is in some dumb over commercial  vampire  it has about as much to do with paris as old ladies  in the  fantasy shows up i think first in the  brothers segment uh thanks j e for ruining another movie and then makes way too many appearances the point of being in paris is that you don't need make believe crap to make your days extraordinary why  it by neighborhood if  de la madeleine is  with vampires for some loser director has there ever been a genre more over represented than the vampire film every three years we get the same lame vampire clichÃ©s br br making things worse is that the switch from segment to segment is pretty  the transitions get lost this doesn't feel intentional it feels sloppy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGABrJoVZe6"
      },
      "source": [
        "Get the sentiment for the above sentence\n",
        "- positive (1)\n",
        "- negative (0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDyQGJT0Ve-a",
        "outputId": "0fc3fc82-52cc-4d9a-b724-0697ee82c2a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if y_train[6]:\n",
        "  review='Positive'\n",
        "else:\n",
        "  review='Negative'\n",
        "print('Training Label Value:',review)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Label Value: Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDVI98t_RqUu"
      },
      "source": [
        "Sample Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n1WeWLoRp1W",
        "outputId": "a710d20f-d866-4652-badf-7cdb9806ecb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoded = \" \".join( [reverse_index.get(i - 3, \"\") for i in X_test[6]] )\n",
        "print('Sample Feature value of Test Data:') \n",
        "print(decoded.lstrip())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Feature value of Test Data:\n",
            "i'm tired of people judging films on their historical accuracy it's a movie people the writers and directors are supposed to put their own spin into the story there are a number of movies out there that aren't entirely accurate with the history braveheart   gangs of new york  an american legend the last of the  all fantastic films that are mildly inaccurate historically if you want to see a few great actors do what they do best then i suggest you see this film and don't worry about the accuracy of the facts just enjoy the quality of the film the storyline and one of the greatest actors of our time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As7nBVauR6It",
        "outputId": "3b92c6e9-e79c-4e95-ab02-2026455c3bdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if y_test[6]:\n",
        "  review='Positive'\n",
        "else:\n",
        "  review='Negative'\n",
        "print('Training Label Value:',review)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Label Value: Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCjr8miXIWB"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Define a Sequential Model\n",
        "- Add Embedding layer\n",
        "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
        "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
        "  - Size of the vocabulary will be 10000\n",
        "  - Give dimension of the dense embedding as 100\n",
        "  - Length of input sequences should be 300\n",
        "- Add LSTM layer\n",
        "  - Pass value in `return_sequences` as True\n",
        "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
        "- Add Flatten layer\n",
        "- Add Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5GxT1caFEq",
        "outputId": "9042eb27-3da1-49d1-a2d3-8c27667b4bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Flatten,TimeDistributed,Input\n",
        "vocabulary_size=10000\n",
        "embedding_size=100\n",
        "input_len=300\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=input_len))\n",
        "model.add(LSTM(100,return_sequences=True,activation='elu'))\n",
        "model.add(TimeDistributed(Dense(100, activation='softsign')))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4bknOobDby"
      },
      "source": [
        "### Compile the model (4 Marks)\n",
        "- Use Optimizer as Adam\n",
        "- Use Binary Crossentropy as loss\n",
        "- Use Accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4RJ0CQbwFY"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEzwazqbz3T"
      },
      "source": [
        "### Print model summary (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx1yxwlb2Ue",
        "outputId": "bec0d2b0-ca85-467e-e05e-325d2cb4ad04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 300, 100)          80400     \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 300, 100)          10100     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 30001     \n",
            "=================================================================\n",
            "Total params: 1,120,501\n",
            "Trainable params: 1,120,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkolKP4b-U6"
      },
      "source": [
        "### Fit the model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRg3KFXLcAkk",
        "outputId": "4d3f8825-80dc-4df8-8a6d-0a21df762c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_accuracy',mode='max')\n",
        "model.fit(X_train, y_train,validation_split=0.05, epochs=15, batch_size=64, callbacks=[es])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "557/557 [==============================] - 220s 395ms/step - loss: 0.3277 - accuracy: 0.8516 - val_loss: 0.2395 - val_accuracy: 0.9024\n",
            "Epoch 2/15\n",
            "557/557 [==============================] - 217s 389ms/step - loss: 0.1851 - accuracy: 0.9276 - val_loss: 0.2504 - val_accuracy: 0.8944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4c83ac9dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLl54MXnkEA"
      },
      "source": [
        "### Evaluate model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUqY-bD8RaDR",
        "outputId": "a3bf517e-9cd0-40f5-a507-9859efa4a378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 13s 34ms/step - loss: 0.2625 - accuracy: 0.8967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlEtgPLn0TAy",
        "outputId": "e9fcc678-3ab2-4f14-f339-a806031dcfd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Accuracy of model: {0:.2%}\".format(result[1]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model: 89.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2amr1tJn9Jz"
      },
      "source": [
        "### Predict on one sample (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4idfWR_A8E",
        "outputId": "cd72c551-c274-4959-f98d-d4ed775a9164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred=model.predict(X_test)\n",
        "y_pred_new=y_pred\n",
        "j=0\n",
        "for i in y_pred:\n",
        "  if i<0.5: \n",
        "    y_pred_new[j]=0\n",
        "  else:\n",
        "    y_pred_new[j]=1\n",
        "  j=j+1\n",
        "print('Actual Output: ',y_test[3])\n",
        "print('Predicted Output: ',y_pred[3])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Output:  1\n",
            "Predicted Output:  [1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nl0yF3p57at"
      },
      "source": [
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "accuracy_score_test=metrics.accuracy_score(y_test,y_pred_new)\n",
        "conf_metr=metrics.confusion_matrix(y_test,y_pred_new,labels=[1,0])\n",
        "df_conf_metr=pd.DataFrame(conf_metr,index = [i for i in [\"Actual 1\",\"Actual 0\"]],columns=[i for i in [\"Predict 1\",\"Predict 0\"]])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJervOJoGWPr",
        "outputId": "5061bcdd-5915-4219-8647-5ef30fbcc8f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Confusion Matrix :\")\n",
        "print(df_conf_metr)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "          Predict 1  Predict 0\n",
            "Actual 1       5732        551\n",
            "Actual 0        740       5477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMC33ibKg00"
      },
      "source": [
        "# ***Model Summary***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBjyF5CYKtKa"
      },
      "source": [
        "IMDB Sentiment Classification with LSTM is giving the test accuracy of 89.67. The model has the train accuracy of 92.67 and Val accuracy of 89.44.\n",
        "\n",
        "The model uses Sequential model with layers like Embedding Layer with the vocabulary size of 10000 ,  Embedding size of 100 and input length of 300.\n",
        "Followed by LSTM Layer , TimeDistributes Layer with Dense and activation function as SOftsign, Flatten Layer and Dense layer with activation as Sigmoid.\n",
        "\n",
        "The model has correctly predicted the 5732 Postive Sentiment and 5477 Negative Sentiment.\n",
        "\n",
        "It has also predicted incorrectly 740 as Positive Sentiment and 551 as Negative Sentiment."
      ]
    }
  ]
}